{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import torchvision\n",
    "# from cnn_rnn_models.cnn_rnn_vinalys.dataHandler import get_loader\n",
    "# from data_preprocessing.build_vocab import Vocabulary\n",
    "# from cnn_rnn_models.cnn_rnn_vinalys.model_cnn_rnn import EncoderCNN, DecoderRNN\n",
    "# from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from cnn_rnn_models.cnn_rnn_vinalys.dataHandler import get_loader\n",
    "from data_preprocessing.build_vocab import Vocabulary\n",
    "from cnn_rnn_models.cnn_rnn_vinalys.model_cnn_rnn import EncoderCNN, DecoderRNN\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--model_path MODEL_PATH] [--crop_size CROP_SIZE]\n",
      "                   [--vocab_path VOCAB_PATH] [--image_dir IMAGE_DIR]\n",
      "                   [--caption_path CAPTION_PATH] [--log_step LOG_STEP]\n",
      "                   [--save_step SAVE_STEP] [--embed_size EMBED_SIZE]\n",
      "                   [--hidden_size HIDDEN_SIZE] [--num_layers NUM_LAYERS]\n",
      "                   [--num_epochs NUM_EPOCHS] [--batch_size BATCH_SIZE]\n",
      "                   [--num_workers NUM_WORKERS] [--learning_rate LEARNING_RATE]\n",
      "__main__.py: error: unrecognized arguments: -f C:\\Users\\mehar\\AppData\\Roaming\\jupyter\\runtime\\kernel-baa6d027-d70f-40bf-a452-65ed205c55de.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mehar\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    # Create model directory\n",
    "    if not os.path.exists(args.model_path):\n",
    "        os.makedirs(args.model_path)\n",
    "\n",
    "    # Data augmentation and normalization for training\n",
    "    # Just normalization for validation\n",
    "\n",
    "    # Image preprocessing, normalization for the pretrained resnet\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # Split: val, train, test\n",
    "\n",
    "\n",
    "#     #COCO\n",
    "#     \"\"\"\n",
    "#     cocoimgs = json.load(open('./coco_raw.json', 'r'))\n",
    "#     random.shuffle(cocoimgs)\n",
    "#     len_train = int(round(0.7 * len(cocoimgs)))\n",
    "#     len_val = int(round(0.20 * len(cocoimgs)))\n",
    "\n",
    "#     train_imgs = cocoimgs[:len_train]\n",
    "#     val_imgs = cocoimgs[len_train:len_train + len_val]\n",
    "#     test_imgs = cocoimgs[len_train + len_val:]\n",
    "\n",
    "\n",
    "#     image_datasets = {}\n",
    "#     image_datasets['train'] = train_imgs\n",
    "#     image_datasets['val'] = val_imgs\n",
    "#     image_datasets['test'] = test_imgs\n",
    "    \n",
    "#     \"\"\"\n",
    "#     #Flicker\n",
    "#     \"\"\"\n",
    "#     flicker_trainimgs = json.load(open('./train_imgcap.json', 'r'))\n",
    "#     flicker_testimgs = json.load(open('./test_imgcap.json', 'r'))\n",
    "#     len_train = len(flicker_trainimgs)\n",
    "#     len_val = len(flicker_testimgs)\n",
    "\n",
    "#     image_datasets = {}\n",
    "#     image_datasets['train'] = flicker_trainimgs\n",
    "#     image_datasets['val'] = flicker_testimgs\n",
    "#     #image_datasets['test'] = test_imgs\n",
    "#     \"\"\"\n",
    "#     #Radiology\n",
    "    radimgs = json.load(open('./data_preprocessing/radcap_bodypartsplit_data.json', 'r'))\n",
    "    radimgs_ankle = radimgs['ankle']\n",
    "    radimgs_wrist = radimgs['wrist']\n",
    "    random.shuffle(radimgs_ankle)\n",
    "    random.shuffle(radimgs_wrist)\n",
    "    radimgs_fracture_ankle = []\n",
    "    radimgs_fracture_wrist = []\n",
    "\n",
    "    for jimg in radimgs_ankle:\n",
    "        if int(jimg['Fracture']) > 0 and int(jimg['Implant']) < 0:\n",
    "            # radimgs_fracture.append(jimg)\n",
    "            if 'oförändra' not in jimg['paragraph'][0] and 'Oförändra' not in jimg['paragraph'][0]:\n",
    "                radimgs_fracture_ankle.append(jimg)\n",
    "\n",
    "    for jimg in radimgs_wrist:\n",
    "        if int(jimg['Fracture']) > 0 and int(jimg['Implant']) < 0:\n",
    "            #radimgs_fracture_wrist.append(jimg)\n",
    "            if 'jämfört' not in jimg['paragraph'][0] and 'Jämfört' not in jimg['paragraph'][0] and 'NA' not in jimg['paragraph'][0] and 'Na' not in jimg['paragraph'][0] and 'na' not in jimg['paragraph'][0]:\n",
    "                radimgs_fracture_wrist.append(jimg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    len_train = int(round(0.7 * len(radimgs_fracture_wrist)))\n",
    "    len_val = int(round(0.20 * len(radimgs_fracture_wrist)))\n",
    "\n",
    "    train_imgs = radimgs_fracture_wrist[:len_train]\n",
    "    val_imgs = radimgs_fracture_wrist[len_train:len_train + len_val]\n",
    "    test_imgs = radimgs_fracture_wrist[len_train + len_val:]\n",
    "\n",
    "    #json.dump(test_imgs, open('wrist_test_data.json', 'w'))\n",
    "\n",
    "    image_datasets = {}\n",
    "    image_datasets['train'] = train_imgs\n",
    "    image_datasets['val'] = val_imgs\n",
    "    image_datasets['test'] = test_imgs\n",
    "\n",
    "    json.dump(test_imgs, open('wrist_test_data_only_fracture_mostly_without_checkup.json', 'w'))\n",
    "\n",
    "    #Image preprocessing, normalization for the pretrained resnet\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(args.crop_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    # Load vocabulary wrapper\n",
    "    with open(args.vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "\n",
    "    #with open('data.json') as json_file:\n",
    "    #    json_data = json.load(json_file)\n",
    "    #vocab = json_data['ix_to_word']\n",
    "\n",
    "\n",
    "\n",
    "    # Build data loader\n",
    "    dataloaders = {x: get_loader(args.image_dir, image_datasets[x], vocab, data_transforms['train'], args.batch_size, shuffle=True,\n",
    "                                 num_workers=args.num_workers)\n",
    "                   for x in ['train', 'val']}\n",
    "\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "    #class_names = image_datasets['train'].classes\n",
    "\n",
    "    # Build the models\n",
    "    encoder = EncoderCNN(args.embed_size).to(device)\n",
    "    decoder = DecoderRNN(args.embed_size, args.hidden_size, len(vocab), args.num_layers).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "    #params = list(decoder.parameters()) + list(encoder.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=args.learning_rate)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "    # Train the models\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_encoder_wts = copy.deepcopy(encoder.state_dict())\n",
    "    best_model_decoder_wts = copy.deepcopy(decoder.state_dict())\n",
    "    lowest_loss = 100000\n",
    "\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, args.num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            total_step = len(dataloaders['train'])\n",
    "            if phase == 'train':\n",
    "                exp_lr_scheduler.step()\n",
    "                encoder.train()  # Set model to training mode\n",
    "            else:\n",
    "                encoder.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, (images, captions, lengths) in enumerate(dataloaders[phase]):\n",
    "                # Set mini-batch dataset\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Forward, backward and optimize\n",
    "                    features = encoder(images)\n",
    "                    outputs = decoder(features, captions, lengths)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    decoder.zero_grad()\n",
    "                    encoder.zero_grad()\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \"\"\"\n",
    "                # Print log info\n",
    "                if i % args.log_step == 0:\n",
    "                    print('i: %d' % i)\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                        .format(epoch, args.num_epochs, i, total_step, loss.item(), np.exp(loss.item())))\n",
    "                    \n",
    "                \"\"\"\n",
    "            # statistics\n",
    "            running_loss += loss.item()*images.size(0)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < lowest_loss:\n",
    "                lowest_loss = epoch_loss\n",
    "                best_model_encoder_wts = copy.deepcopy(encoder.state_dict())\n",
    "                best_model_decoder_wts = copy.deepcopy(decoder.state_dict())\n",
    "        print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Lowest loss: {:4f}'.format(lowest_loss))\n",
    "\n",
    "        \n",
    "\n",
    "    # Save the model checkpoints\n",
    "    torch.save(best_model_decoder_wts, os.path.join(\n",
    "        args.model_path, 'decoder-{}-{}.ckpt'.format('Vinalys', 'wrist-end')))\n",
    "    torch.save(best_model_encoder_wts, os.path.join(\n",
    "        args.model_path, 'encoder-{}-{}.ckpt'.format('Vinalys', 'wrist-end')))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_path', type=str, default='./models/', help='path for saving trained models')\n",
    "    parser.add_argument('--crop_size', type=int, default=224, help='size for randomly cropping images')\n",
    "    parser.add_argument('--vocab_path', type=str, default='./data/vocab.pkl', help='path for vocabulary wrapper')\n",
    "    parser.add_argument('--image_dir', type=str, default='./data/', help='directory for resized images')\n",
    "    parser.add_argument('--caption_path', type=str, default='train_imgcap.json',\n",
    "                        help='path for train annotation json file')\n",
    "    parser.add_argument('--log_step', type=int, default=10, help='step size for prining log info')\n",
    "    parser.add_argument('--save_step', type=int, default=1000, help='step size for saving trained models')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--embed_size', type=int, default=256, help='dimension of word embedding vectors')\n",
    "    parser.add_argument('--hidden_size', type=int, default=512, help='dimension of lstm hidden states')\n",
    "    parser.add_argument('--num_layers', type=int, default=1, help='number of layers in lstm')\n",
    "\n",
    "    parser.add_argument('--num_epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch_size', type=int, default=20)\n",
    "    parser.add_argument('--num_workers', type=int, default=2)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001)\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
